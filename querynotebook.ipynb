{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2a1f2fc-cd55-48b5-affa-c357e2665bba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Query 1. - Add a column named as \"\"Revenue\"\" into dataframe which is the sum of the below columns 'Fare_amount','Extra','MTA_tax','Improvement_surcharge','Tip_amount','Tolls_amount','Total_amount' \n",
    "\n",
    "Query 2. - Increasing count of total passengers in New York City by area \n",
    "\n",
    "Query 3. - Realtime Average fare/total earning amount earned by 2 vendors \n",
    "\n",
    "Query 4. - Moving Count of payments made by each payment mode \n",
    "\n",
    "Query 5. - Highest two gaining vendor's on a particular date with no of passenger and total distance by cab \n",
    "\n",
    "Query 6. - Most no of passenger between a route of two location. \n",
    "\n",
    "Query 7. - Get top pickup locations with most passengers in last 5/10 seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a729c0f7-12e8-4d37-919e-c108c0de5a24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.azure.account.auth.type.storageaccountlast23.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.storageaccountlast23.dfs.core.windows.net\", \n",
    "               \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.storageaccountlast23.dfs.core.windows.net\", \"73ab7fe9-3734-40ec-b7a7-c2057cccc029\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.storageaccountlast23.dfs.core.windows.net\", \"Z4U8Q~fzB.h3Avz3YOcywkxVIrPiLJTVsTgnMbXp\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.storageaccountlast23.dfs.core.windows.net\", \n",
    "               \"https://login.microsoftonline.com/866aa9ca-982d-4970-86dc-8e2c0a9ad24b/oauth2/token\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfd4f21d-00c0-4eef-b607-83fbbbc540c0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read the parquet file from local path"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n|       1| 2018-01-01 00:21:05|  2018-01-01 00:24:23|              1|          0.5|         1|                 N|          41|          24|           2|        4.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         5.8|                null|       null|\n|       1| 2018-01-01 00:44:55|  2018-01-01 01:03:05|              1|          2.7|         1|                 N|         239|         140|           2|       14.0|  0.5|    0.5|       0.0|         0.0|                  0.3|        15.3|                null|       null|\n|       1| 2018-01-01 00:08:26|  2018-01-01 00:14:21|              2|          0.8|         1|                 N|         262|         141|           1|        6.0|  0.5|    0.5|       1.0|         0.0|                  0.3|         8.3|                null|       null|\n|       1| 2018-01-01 00:20:22|  2018-01-01 00:52:51|              1|         10.2|         1|                 N|         140|         257|           2|       33.5|  0.5|    0.5|       0.0|         0.0|                  0.3|        34.8|                null|       null|\n|       1| 2018-01-01 00:09:18|  2018-01-01 00:27:06|              2|          2.5|         1|                 N|         246|         239|           1|       12.5|  0.5|    0.5|      2.75|         0.0|                  0.3|       16.55|                null|       null|\n+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\nonly showing top 5 rows\n\nroot\n |-- VendorID: long (nullable = true)\n |-- tpep_pickup_datetime: timestamp (nullable = true)\n |-- tpep_dropoff_datetime: timestamp (nullable = true)\n |-- passenger_count: long (nullable = true)\n |-- trip_distance: double (nullable = true)\n |-- RatecodeID: long (nullable = true)\n |-- store_and_fwd_flag: string (nullable = true)\n |-- PULocationID: long (nullable = true)\n |-- DOLocationID: long (nullable = true)\n |-- payment_type: long (nullable = true)\n |-- fare_amount: double (nullable = true)\n |-- extra: double (nullable = true)\n |-- mta_tax: double (nullable = true)\n |-- tip_amount: double (nullable = true)\n |-- tolls_amount: double (nullable = true)\n |-- improvement_surcharge: double (nullable = true)\n |-- total_amount: double (nullable = true)\n |-- congestion_surcharge: double (nullable = true)\n |-- airport_fee: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#waiting 1 - 2 minutes to updates the access...\n",
    "df = spark.read.parquet(\"abfss://nycdata@storageaccountlast23.dfs.core.windows.net/yellow_tripdata_2018-01.parquet\")\n",
    "df.show(5)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29ee2ff7-6778-461b-9e18-1ebccdf31d5a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Add revenue column"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n|fare_amount|Revenue|\n+-----------+-------+\n|        4.5|   11.6|\n|       14.0|   30.6|\n|        6.0|   16.6|\n|       33.5|   69.6|\n|       12.5|   33.1|\n+-----------+-------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumn(\"Revenue\", \n",
    "    col(\"fare_amount\") + col(\"extra\") + col(\"mta_tax\") +\n",
    "    col(\"improvement_surcharge\") + col(\"tip_amount\") +\n",
    "    col(\"tolls_amount\") + col(\"total_amount\")\n",
    ")\n",
    "df.select(\"fare_amount\", \"Revenue\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e067198-38e3-4d53-8197-d5fbb5635b18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+\n|PULocationID|total_passengers|\n+------------+----------------+\n|         161|          573916|\n|         237|          572988|\n|         236|          550847|\n|         230|          511603|\n|         162|          490738|\n|         186|          460129|\n|         234|          455335|\n|         170|          440565|\n|          48|          438312|\n|         142|          427097|\n|         163|          391685|\n|          79|          390878|\n|         138|          381519|\n|         239|          370578|\n|         141|          340100|\n|         132|          333826|\n|         164|          332941|\n|         107|          330597|\n|          68|          324889|\n|         100|          296869|\n+------------+----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"PULocationID\")\\\n",
    "  .sum(\"passenger_count\")\\\n",
    "  .withColumnRenamed(\"sum(passenger_count)\", \"total_passengers\")\\\n",
    "  .orderBy(\"total_passengers\", ascending=False)\\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aec5441c-1bf7-4d82-985e-f4adc7713a79",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Average fare and revenue by vendor"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+------------------+\n|VendorID|          avg_fare|       avg_revenue|\n+--------+------------------+------------------+\n|       1|11.959975333672647|30.254768579804274|\n|       2|12.467037370438032|31.545275988143338|\n+--------+------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"VendorID\")\\\n",
    "  .agg({\"fare_amount\": \"avg\", \"Revenue\": \"avg\"})\\\n",
    "  .withColumnRenamed(\"avg(fare_amount)\", \"avg_fare\")\\\n",
    "  .withColumnRenamed(\"avg(Revenue)\", \"avg_revenue\")\\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "364e9f54-65f0-4555-bffd-a7768fca7ab6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Movie count of payments by mode"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n|payment_type|moving_count|\n+------------+------------+\n|           1|           1|\n|           1|           2|\n|           1|           3|\n|           1|           4|\n|           1|           5|\n|           1|           6|\n|           1|           7|\n|           1|           8|\n|           1|           9|\n|           1|          10|\n|           1|          11|\n|           1|          11|\n|           1|          11|\n|           1|          11|\n|           1|          11|\n|           1|          11|\n|           1|          11|\n|           1|          11|\n|           1|          11|\n|           1|          11|\n+------------+------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "windowSpec = Window.partitionBy(\"payment_type\").orderBy(\"tpep_pickup_datetime\").rowsBetween(-10, 0)\n",
    "\n",
    "df = df.withColumn(\"moving_count\", count(\"payment_type\").over(windowSpec))\n",
    "df.select(\"payment_type\", \"moving_count\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a0b7955-3ba6-4715-b34b-b57e156a7173",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Top 5 vendors by revenue on a particular date"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+------------------+----------------+------------------+\n| trip_date|VendorID|     total_revenue|total_passengers|    total_distance|\n+----------+--------+------------------+----------------+------------------+\n|2018-01-15|       2|4475021.5800031945|          266559| 439143.8800000009|\n|2018-01-15|       1|3137794.8000030015|          130018|300357.29999999865|\n+----------+--------+------------------+----------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, sum\n",
    "\n",
    "df = df.withColumn(\"trip_date\", to_date(\"tpep_pickup_datetime\"))\n",
    "\n",
    "df.groupBy(\"trip_date\", \"VendorID\")\\\n",
    "  .agg(\n",
    "      sum(\"Revenue\").alias(\"total_revenue\"),\n",
    "      sum(\"passenger_count\").alias(\"total_passengers\"),\n",
    "      sum(\"trip_distance\").alias(\"total_distance\")\n",
    "  )\\\n",
    "  .filter(\"trip_date = '2018-01-15'\")\\\n",
    "  .orderBy(\"total_revenue\", ascending=False)\\\n",
    "  .show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4506d1cc-486a-44b6-8f3e-4c41245cca33",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Route with most passengers"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+----------------+\n|PULocationID|DOLocationID|total_passengers|\n+------------+------------+----------------+\n|         264|         264|          186705|\n+------------+------------+----------------+\nonly showing top 1 row\n\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"PULocationID\", \"DOLocationID\")\\\n",
    "  .sum(\"passenger_count\")\\\n",
    "  .withColumnRenamed(\"sum(passenger_count)\", \"total_passengers\")\\\n",
    "  .orderBy(\"total_passengers\", ascending=False)\\\n",
    "  .show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ef02aae-7d3a-4e26-924b-af60301022b9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "top pickup locations in last 10 seconds(Simulated)"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+\n|              window|PULocationID|sum(passenger_count)|\n+--------------------+------------+--------------------+\n|{2018-01-23 20:52...|         230|                  34|\n|{2018-01-18 23:38...|         230|                  31|\n|{2018-01-11 21:00...|         161|                  31|\n|{2018-01-17 18:19...|         237|                  29|\n|{2018-01-22 17:14...|         237|                  29|\n|{2018-01-24 21:45...|         162|                  29|\n|{2018-01-18 23:20...|         142|                  28|\n|{2018-01-06 15:15...|         264|                  28|\n|{2018-01-24 22:00...|         161|                  28|\n|{2018-01-21 01:18...|          79|                  28|\n|{2018-01-26 09:12...|         186|                  27|\n|{2018-01-24 21:56...|         161|                  27|\n|{2018-01-29 21:18...|         162|                  27|\n|{2018-01-01 18:51...|         186|                  27|\n|{2018-01-08 15:15...|         264|                  27|\n|{2018-01-25 21:48...|         162|                  27|\n|{2018-01-15 18:58...|         138|                  27|\n|{2018-01-24 21:57...|         142|                  27|\n|{2018-01-10 20:15...|         162|                  27|\n|{2018-01-11 20:09...|         161|                  27|\n+--------------------+------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import window\n",
    "\n",
    "df.withWatermark(\"tpep_pickup_datetime\", \"30 seconds\")\\\n",
    "  .groupBy(window(\"tpep_pickup_datetime\", \"10 seconds\"), \"PULocationID\")\\\n",
    "  .sum(\"passenger_count\")\\\n",
    "  .orderBy(\"sum(passenger_count)\", ascending=False)\\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2e383c6-e556-4f3b-a4f6-eb476d249d8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24a40ca9-d1b3-4456-a9da-fd4820beacee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-07-13 09:28:38",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}